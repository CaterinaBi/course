# Modelli encoder

<Youtube id="MUqNwgPjJvQ" />

I modelli encoder utilizzano solo l'encoder di un modello Transformer. In ogni fase, the attention layers can access all the words in the initial sentence. These models are often characterized as having "bi-directional" attention, e vengono spesso chiamati *auto-encoding models*.

The pretraining of these models usually revolves around somehow corrupting a given sentence (for instance, by masking random words in it) and tasking the model with finding or reconstructing the initial sentence.

Encoder models are best suited for tasks requiring an understanding of the full sentence, such as sentence classification, named entity recognition (and more generally word classification), and extractive question answering.

Alcuni esempi di modelli di questo tipo includono:

- [ALBERT](https://huggingface.co/transformers/model_doc/albert.html)
- [BERT](https://huggingface.co/transformers/model_doc/bert.html)
- [DistilBERT](https://huggingface.co/transformers/model_doc/distilbert.html)
- [ELECTRA](https://huggingface.co/transformers/model_doc/electra.html)
- [RoBERTa](https://huggingface.co/transformers/model_doc/roberta.html)
