# Modelli decoder

<Youtube id="d_ixlCubqQw" />

I modelli decoder utilizzano solo il decoder di un modello Transformer. Ad ogni passaggio e per una data parola, i layers di attenzione hanno accesso solo alle parole che la precedono nella frase. These models are often called *auto-regressive models*.

The pretraining of decoder models usually revolves around predicting the next word in the sentence.

Questi modelli sono particolarmente adattu a compiti che hanno a che fare con la generazione testuale.

Alcuni rappresentanti di questa famiglia includono:

- [CTRL](https://huggingface.co/transformers/model_doc/ctrl.html)
- [GPT](https://huggingface.co/transformers/model_doc/gpt.html)
- [GPT-2](https://huggingface.co/transformers/model_doc/gpt2.html)
- [Transformer XL](https://huggingface.co/transformers/model_doc/transfo-xl.html)
